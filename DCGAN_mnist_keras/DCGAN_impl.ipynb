{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep Convolutional Generative Adversarial Network (DCGAN) implementation on MNIST data set using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is an implementation of DCGAN as described in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) on [MNIST](http://yann.lecun.com/exdb/mnist/) data set using [Keras](https://keras.io/) library with [Tensorflow](https://www.tensorflow.org/) backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape, Input, Flatten\n",
    "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import keras.models as km\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set the format of the image data, channels last, to avoid any discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining the Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided transpose convolution, batch normalization, and rectified nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weight_initializer = 'truncated_normal'\n",
    "\n",
    "def generator_model(z_size):\n",
    "    gen_input = Input(shape=(z_size,))\n",
    "    \n",
    "    gen_hidden = Dense(units=7*7*128, \n",
    "                       kernel_initializer=weight_initializer)(gen_input)\n",
    "    gen_hidden = BatchNormalization()(gen_hidden)\n",
    "    gen_hidden = Activation('relu')(gen_hidden)\n",
    "    gen_hidden = Reshape([7, 7, 128])(gen_hidden)\n",
    "    # now we have 7x7x128 image\n",
    "    \n",
    "    gen_hidden = Conv2DTranspose(filters=64,\n",
    "                                 kernel_size=[5, 5],\n",
    "                                 strides=[2, 2],\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=weight_initializer)(gen_hidden)\n",
    "    gen_hidden = BatchNormalization()(gen_hidden)\n",
    "    gen_hidden = Activation('relu')(gen_hidden)\n",
    "    # now we have 14x14x64 image\n",
    "    \n",
    "    gen_hidden = Conv2DTranspose(filters=1,\n",
    "                                 kernel_size=[5, 5],\n",
    "                                 strides=[2, 2],\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=weight_initializer)(gen_hidden)\n",
    "    gen_output = Activation('tanh')(gen_hidden)\n",
    "    # now we have 28x28x1 image\n",
    "    \n",
    "    gen_model = km.Model(inputs=gen_input, outputs=gen_output)\n",
    "    \n",
    "    return gen_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The discriminator network takes as input a 28x28 image and transforms it into a single valued probability of being generated from real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    discr_input = Input(shape=(28,28,1))\n",
    "    \n",
    "    discr_l1 = Conv2D(filters=64,\n",
    "                      kernel_size=[5, 5],\n",
    "                      strides=[2, 2],\n",
    "                      padding='same',\n",
    "                      kernel_initializer=weight_initializer)\n",
    "    discr_hidden1 = discr_l1(discr_input)\n",
    "    discr_l2 = LeakyReLU(alpha=0.2)\n",
    "    discr_hidden2 = discr_l2(discr_hidden1)\n",
    "    \n",
    "    discr_l3 = Conv2D(filters=128,\n",
    "                      kernel_size=[5, 5],\n",
    "                      strides=[2, 2],\n",
    "                      padding='same',\n",
    "                      kernel_initializer=weight_initializer)\n",
    "    discr_hidden3 = discr_l3(discr_hidden2)\n",
    "    discr_l4 = BatchNormalization()\n",
    "    #discr_hidden4 = discr_l4(discr_hidden3)\n",
    "    discr_l5 = LeakyReLU(alpha=0.2)\n",
    "    discr_hidden5 = discr_l5(discr_hidden3)\n",
    "    \n",
    "    discr_l6 = Flatten()\n",
    "    discr_hidden6 = discr_l6(discr_hidden5)\n",
    "    discr_l7 = Dense(units=1, \n",
    "                     kernel_initializer=weight_initializer, \n",
    "                     activation='sigmoid')\n",
    "    discr_output = discr_l7(discr_hidden6)\n",
    "    \n",
    "    discr_model = km.Model(inputs=discr_input, outputs=discr_output)\n",
    "    \n",
    "    return (discr_model, discr_l1, discr_l2, discr_l3, discr_l4, discr_l5, discr_l6, discr_l7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We tune hyperparameters of the Adam optimizer as suggested in the [paper](https://arxiv.org/abs/1511.06434)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "adam_optimizer = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is a helper function to produce a combine image consisting of several generated images (in matrix form). We want just see, how the generator produces better results after several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    #print(generated_images.shape)\n",
    "    num = generated_images.shape[0]\n",
    "    \n",
    "    num_cols = int(math.sqrt(num))\n",
    "    num_rows = int(math.ceil(float(num)/num_cols))\n",
    "    \n",
    "    single_width = generated_images.shape[1]\n",
    "    single_height = generated_images.shape[2]\n",
    "    \n",
    "    image = np.zeros((num_rows*single_height,\n",
    "                      num_cols*single_width),\n",
    "                      dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / num_cols)\n",
    "        j = index % num_cols\n",
    "        image[i*single_height:(i+1)*single_height, j*single_width:(j+1)*single_width] = img[:, :, 0]\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Connect everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we define inputs, outputs and compile the model. Pay attention that we need to combine generator and discriminator models but for this combination the discriminator must not be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_trainable(model, trainable):\n",
    "    model.trainable = trainable\n",
    "    for l in model.layers:\n",
    "        l.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_size = 100\n",
    "\n",
    "# compile generator\n",
    "generator = generator_model(z_size)\n",
    "generator.compile(optimizer=adam_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create discriminator model\n",
    "discriminator, discr_l1, discr_l2, discr_l3, discr_l4, discr_l5, discr_l6, discr_l7 = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "make_trainable(discriminator, False) # !!! here discriminator must not be trainable\n",
    "\n",
    "# create models that consist of generator and discriminator and compile it\n",
    "gan_input = Input(shape=(z_size,))\n",
    "gan_hidden = generator(gan_input)\n",
    "gan_hidden = discr_l1(gan_hidden)\n",
    "gan_hidden = discr_l2(gan_hidden)\n",
    "gan_hidden = discr_l3(gan_hidden)\n",
    "#gan_hidden = discr_l4(gan_hidden)\n",
    "gan_hidden = discr_l5(gan_hidden)\n",
    "gan_hidden = discr_l6(gan_hidden)\n",
    "gan_output = discr_l7(gan_hidden)\n",
    "\n",
    "gan = km.Model(inputs=gan_input,outputs=gan_output)\n",
    "gan.compile(optimizer=adam_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compile discriminator\n",
    "make_trainable(discriminator, True)\n",
    "discriminator.compile(optimizer=adam_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Prepare train images - we need to squash them to have values (-1, 1) and reshape into 28x28x1 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create directory to save generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GEN_IMAGES_DIR = 'gen_images'\n",
    "if not os.path.exists(GEN_IMAGES_DIR):\n",
    "    os.makedirs(GEN_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25... Batch 1/468... Discriminator Loss: 0.7413... Generator Loss: 0.7678\n",
      "Epoch 1/25... Batch 2/468... Discriminator Loss: 0.6440... Generator Loss: 0.6379\n",
      "Epoch 1/25... Batch 3/468... Discriminator Loss: 0.5732... Generator Loss: 0.5488\n",
      "Epoch 1/25... Batch 4/468... Discriminator Loss: 0.5593... Generator Loss: 0.5040\n",
      "Epoch 1/25... Batch 5/468... Discriminator Loss: 0.5559... Generator Loss: 0.5147\n",
      "Epoch 1/25... Batch 6/468... Discriminator Loss: 0.5543... Generator Loss: 0.5569\n",
      "Epoch 1/25... Batch 7/468... Discriminator Loss: 0.5454... Generator Loss: 0.6263\n",
      "Epoch 1/25... Batch 8/468... Discriminator Loss: 0.5392... Generator Loss: 0.6733\n",
      "Epoch 1/25... Batch 9/468... Discriminator Loss: 0.5334... Generator Loss: 0.7097\n",
      "Epoch 1/25... Batch 10/468... Discriminator Loss: 0.5267... Generator Loss: 0.7527\n",
      "Epoch 1/25... Batch 11/468... Discriminator Loss: 0.5174... Generator Loss: 0.8209\n",
      "Epoch 1/25... Batch 12/468... Discriminator Loss: 0.5123... Generator Loss: 0.9093\n",
      "Epoch 1/25... Batch 13/468... Discriminator Loss: 0.5011... Generator Loss: 0.9821\n",
      "Epoch 1/25... Batch 14/468... Discriminator Loss: 0.4932... Generator Loss: 1.0061\n",
      "Epoch 1/25... Batch 15/468... Discriminator Loss: 0.4876... Generator Loss: 1.0605\n",
      "Epoch 1/25... Batch 16/468... Discriminator Loss: 0.4788... Generator Loss: 1.1393\n",
      "Epoch 1/25... Batch 17/468... Discriminator Loss: 0.4835... Generator Loss: 0.9724\n",
      "Epoch 1/25... Batch 18/468... Discriminator Loss: 0.5142... Generator Loss: 0.7272\n",
      "Epoch 1/25... Batch 19/468... Discriminator Loss: 0.5345... Generator Loss: 0.6334\n",
      "Epoch 1/25... Batch 20/468... Discriminator Loss: 0.5306... Generator Loss: 0.7168\n",
      "Epoch 1/25... Batch 21/468... Discriminator Loss: 0.5067... Generator Loss: 0.9202\n",
      "Epoch 1/25... Batch 22/468... Discriminator Loss: 0.4807... Generator Loss: 1.0976\n",
      "Epoch 1/25... Batch 23/468... Discriminator Loss: 0.4531... Generator Loss: 1.1662\n",
      "Epoch 1/25... Batch 24/468... Discriminator Loss: 0.4393... Generator Loss: 1.1098\n",
      "Epoch 1/25... Batch 25/468... Discriminator Loss: 0.4339... Generator Loss: 1.0278\n",
      "Epoch 1/25... Batch 26/468... Discriminator Loss: 0.4315... Generator Loss: 1.0099\n",
      "Epoch 1/25... Batch 27/468... Discriminator Loss: 0.4314... Generator Loss: 0.9340\n",
      "Epoch 1/25... Batch 28/468... Discriminator Loss: 0.4170... Generator Loss: 0.9252\n",
      "Epoch 1/25... Batch 29/468... Discriminator Loss: 0.4113... Generator Loss: 0.9430\n",
      "Epoch 1/25... Batch 30/468... Discriminator Loss: 0.4047... Generator Loss: 0.8923\n",
      "Epoch 1/25... Batch 31/468... Discriminator Loss: 0.3929... Generator Loss: 0.9876\n",
      "Epoch 1/25... Batch 32/468... Discriminator Loss: 0.3799... Generator Loss: 1.0025\n",
      "Epoch 1/25... Batch 33/468... Discriminator Loss: 0.3780... Generator Loss: 0.7630\n",
      "Epoch 1/25... Batch 34/468... Discriminator Loss: 0.3642... Generator Loss: 1.0247\n",
      "Epoch 1/25... Batch 35/468... Discriminator Loss: 0.3542... Generator Loss: 0.8526\n",
      "Epoch 1/25... Batch 36/468... Discriminator Loss: 0.3381... Generator Loss: 1.1185\n",
      "Epoch 1/25... Batch 37/468... Discriminator Loss: 0.3336... Generator Loss: 0.9611\n",
      "Epoch 1/25... Batch 38/468... Discriminator Loss: 0.3319... Generator Loss: 0.6769\n",
      "Epoch 1/25... Batch 39/468... Discriminator Loss: 0.3221... Generator Loss: 1.3728\n",
      "Epoch 1/25... Batch 40/468... Discriminator Loss: 0.2958... Generator Loss: 0.9315\n",
      "Epoch 1/25... Batch 41/468... Discriminator Loss: 0.2849... Generator Loss: 1.2849\n",
      "Epoch 1/25... Batch 42/468... Discriminator Loss: 0.2812... Generator Loss: 0.8049\n",
      "Epoch 1/25... Batch 43/468... Discriminator Loss: 0.2783... Generator Loss: 1.3141\n",
      "Epoch 1/25... Batch 44/468... Discriminator Loss: 0.2680... Generator Loss: 0.4565\n",
      "Epoch 1/25... Batch 45/468... Discriminator Loss: 0.2758... Generator Loss: 0.9510\n",
      "Epoch 1/25... Batch 46/468... Discriminator Loss: 0.2550... Generator Loss: 0.7079\n",
      "Epoch 1/25... Batch 47/468... Discriminator Loss: 0.2471... Generator Loss: 0.9364\n",
      "Epoch 1/25... Batch 48/468... Discriminator Loss: 0.2425... Generator Loss: 0.7036\n",
      "Epoch 1/25... Batch 49/468... Discriminator Loss: 0.2344... Generator Loss: 0.9448\n",
      "Epoch 1/25... Batch 50/468... Discriminator Loss: 0.2429... Generator Loss: 0.3220\n",
      "Epoch 1/25... Batch 51/468... Discriminator Loss: 0.2545... Generator Loss: 0.6277\n",
      "Epoch 1/25... Batch 52/468... Discriminator Loss: 0.2374... Generator Loss: 0.5006\n",
      "Epoch 1/25... Batch 53/468... Discriminator Loss: 0.2419... Generator Loss: 0.2473\n",
      "Epoch 1/25... Batch 54/468... Discriminator Loss: 0.2537... Generator Loss: 1.1255\n",
      "Epoch 1/25... Batch 55/468... Discriminator Loss: 0.2995... Generator Loss: 0.0202\n",
      "Epoch 1/25... Batch 56/468... Discriminator Loss: 0.4587... Generator Loss: 0.2275\n",
      "Epoch 1/25... Batch 57/468... Discriminator Loss: 0.2579... Generator Loss: 1.9563\n",
      "Epoch 1/25... Batch 58/468... Discriminator Loss: 0.5186... Generator Loss: 0.0123\n",
      "Epoch 1/25... Batch 59/468... Discriminator Loss: 0.5646... Generator Loss: 0.0196\n",
      "Epoch 1/25... Batch 60/468... Discriminator Loss: 0.5066... Generator Loss: 0.2930\n",
      "Epoch 1/25... Batch 61/468... Discriminator Loss: 0.2557... Generator Loss: 1.6028\n",
      "Epoch 1/25... Batch 62/468... Discriminator Loss: 0.3161... Generator Loss: 0.6948\n",
      "Epoch 1/25... Batch 63/468... Discriminator Loss: 0.2333... Generator Loss: 0.4521\n",
      "Epoch 1/25... Batch 64/468... Discriminator Loss: 0.2431... Generator Loss: 0.6297\n",
      "Epoch 1/25... Batch 65/468... Discriminator Loss: 0.2352... Generator Loss: 0.8957\n",
      "Epoch 1/25... Batch 66/468... Discriminator Loss: 0.2288... Generator Loss: 0.9479\n",
      "Epoch 1/25... Batch 67/468... Discriminator Loss: 0.2302... Generator Loss: 1.1484\n",
      "Epoch 1/25... Batch 68/468... Discriminator Loss: 0.2439... Generator Loss: 0.8684\n",
      "Epoch 1/25... Batch 69/468... Discriminator Loss: 0.2331... Generator Loss: 0.7431\n",
      "Epoch 1/25... Batch 70/468... Discriminator Loss: 0.2338... Generator Loss: 0.7769\n",
      "Epoch 1/25... Batch 71/468... Discriminator Loss: 0.2342... Generator Loss: 0.9393\n",
      "Epoch 1/25... Batch 72/468... Discriminator Loss: 0.2469... Generator Loss: 0.6078\n",
      "Epoch 1/25... Batch 73/468... Discriminator Loss: 0.2437... Generator Loss: 1.0498\n",
      "Epoch 1/25... Batch 74/468... Discriminator Loss: 0.2412... Generator Loss: 1.1486\n",
      "Epoch 1/25... Batch 75/468... Discriminator Loss: 0.2399... Generator Loss: 1.0496\n",
      "Epoch 1/25... Batch 76/468... Discriminator Loss: 0.2273... Generator Loss: 1.4146\n",
      "Epoch 1/25... Batch 77/468... Discriminator Loss: 0.2349... Generator Loss: 1.0579\n",
      "Epoch 1/25... Batch 78/468... Discriminator Loss: 0.2349... Generator Loss: 1.2857\n",
      "Epoch 1/25... Batch 79/468... Discriminator Loss: 0.2320... Generator Loss: 1.6876\n",
      "Epoch 1/25... Batch 80/468... Discriminator Loss: 0.2513... Generator Loss: 1.1017\n",
      "Epoch 1/25... Batch 81/468... Discriminator Loss: 0.2654... Generator Loss: 1.8781\n",
      "Epoch 1/25... Batch 82/468... Discriminator Loss: 0.2358... Generator Loss: 1.6055\n",
      "Epoch 1/25... Batch 83/468... Discriminator Loss: 0.2356... Generator Loss: 1.0860\n",
      "Epoch 1/25... Batch 84/468... Discriminator Loss: 0.2486... Generator Loss: 1.1949\n",
      "Epoch 1/25... Batch 85/468... Discriminator Loss: 0.2383... Generator Loss: 2.2681\n",
      "Epoch 1/25... Batch 86/468... Discriminator Loss: 0.2577... Generator Loss: 1.0669\n",
      "Epoch 1/25... Batch 87/468... Discriminator Loss: 0.2452... Generator Loss: 1.6017\n",
      "Epoch 1/25... Batch 88/468... Discriminator Loss: 0.2195... Generator Loss: 2.1734\n",
      "Epoch 1/25... Batch 89/468... Discriminator Loss: 0.2460... Generator Loss: 0.8019\n",
      "Epoch 1/25... Batch 90/468... Discriminator Loss: 0.2634... Generator Loss: 2.0552\n",
      "Epoch 1/25... Batch 91/468... Discriminator Loss: 0.2375... Generator Loss: 1.3282\n",
      "Epoch 1/25... Batch 92/468... Discriminator Loss: 0.2343... Generator Loss: 1.9374\n",
      "Epoch 1/25... Batch 93/468... Discriminator Loss: 0.2288... Generator Loss: 1.7802\n",
      "Epoch 1/25... Batch 94/468... Discriminator Loss: 0.2298... Generator Loss: 1.8240\n",
      "Epoch 1/25... Batch 95/468... Discriminator Loss: 0.2315... Generator Loss: 1.6781\n",
      "Epoch 1/25... Batch 96/468... Discriminator Loss: 0.2287... Generator Loss: 1.8996\n",
      "Epoch 1/25... Batch 97/468... Discriminator Loss: 0.2428... Generator Loss: 1.1123\n",
      "Epoch 1/25... Batch 98/468... Discriminator Loss: 0.2530... Generator Loss: 2.4966\n",
      "Epoch 1/25... Batch 99/468... Discriminator Loss: 0.2600... Generator Loss: 0.7477\n",
      "Epoch 1/25... Batch 100/468... Discriminator Loss: 0.2859... Generator Loss: 2.5575\n",
      "Epoch 1/25... Batch 101/468... Discriminator Loss: 0.2730... Generator Loss: 0.7752\n",
      "Epoch 1/25... Batch 102/468... Discriminator Loss: 0.3091... Generator Loss: 2.7592\n",
      "Epoch 1/25... Batch 103/468... Discriminator Loss: 0.2369... Generator Loss: 2.3042\n",
      "Epoch 1/25... Batch 104/468... Discriminator Loss: 0.2327... Generator Loss: 1.0333\n",
      "Epoch 1/25... Batch 105/468... Discriminator Loss: 0.2638... Generator Loss: 2.4175\n",
      "Epoch 1/25... Batch 106/468... Discriminator Loss: 0.2176... Generator Loss: 2.6815\n",
      "Epoch 1/25... Batch 107/468... Discriminator Loss: 0.2901... Generator Loss: 0.3931\n",
      "Epoch 1/25... Batch 108/468... Discriminator Loss: 0.3781... Generator Loss: 2.3610\n",
      "Epoch 1/25... Batch 109/468... Discriminator Loss: 0.2629... Generator Loss: 1.7003\n",
      "Epoch 1/25... Batch 110/468... Discriminator Loss: 0.2430... Generator Loss: 1.0884\n",
      "Epoch 1/25... Batch 111/468... Discriminator Loss: 0.2543... Generator Loss: 1.9880\n",
      "Epoch 1/25... Batch 112/468... Discriminator Loss: 0.2304... Generator Loss: 1.9410\n",
      "Epoch 1/25... Batch 113/468... Discriminator Loss: 0.2445... Generator Loss: 1.0518\n",
      "Epoch 1/25... Batch 114/468... Discriminator Loss: 0.2402... Generator Loss: 1.1658\n",
      "Epoch 1/25... Batch 115/468... Discriminator Loss: 0.2383... Generator Loss: 1.3789\n",
      "Epoch 1/25... Batch 116/468... Discriminator Loss: 0.2389... Generator Loss: 1.0137\n",
      "Epoch 1/25... Batch 117/468... Discriminator Loss: 0.2309... Generator Loss: 1.7393\n",
      "Epoch 1/25... Batch 118/468... Discriminator Loss: 0.2235... Generator Loss: 1.2553\n",
      "Epoch 1/25... Batch 119/468... Discriminator Loss: 0.2191... Generator Loss: 1.1383\n",
      "Epoch 1/25... Batch 120/468... Discriminator Loss: 0.2209... Generator Loss: 1.6649\n",
      "Epoch 1/25... Batch 121/468... Discriminator Loss: 0.2472... Generator Loss: 0.7039\n",
      "Epoch 1/25... Batch 122/468... Discriminator Loss: 0.2855... Generator Loss: 2.0143\n",
      "Epoch 1/25... Batch 123/468... Discriminator Loss: 0.2571... Generator Loss: 0.7587\n",
      "Epoch 1/25... Batch 124/468... Discriminator Loss: 0.2737... Generator Loss: 1.9499\n",
      "Epoch 1/25... Batch 125/468... Discriminator Loss: 0.2424... Generator Loss: 1.1162\n",
      "Epoch 1/25... Batch 126/468... Discriminator Loss: 0.2348... Generator Loss: 1.6394\n",
      "Epoch 1/25... Batch 127/468... Discriminator Loss: 0.2236... Generator Loss: 1.2527\n",
      "Epoch 1/25... Batch 128/468... Discriminator Loss: 0.2199... Generator Loss: 1.2950\n",
      "Epoch 1/25... Batch 129/468... Discriminator Loss: 0.2144... Generator Loss: 1.3313\n",
      "Epoch 1/25... Batch 130/468... Discriminator Loss: 0.2224... Generator Loss: 0.8179\n",
      "Epoch 1/25... Batch 131/468... Discriminator Loss: 0.2322... Generator Loss: 1.5852\n",
      "Epoch 1/25... Batch 132/468... Discriminator Loss: 0.3193... Generator Loss: 0.1249\n",
      "Epoch 1/25... Batch 133/468... Discriminator Loss: 0.6939... Generator Loss: 2.9835\n",
      "Epoch 1/25... Batch 134/468... Discriminator Loss: 0.8712... Generator Loss: 0.1930\n",
      "Epoch 1/25... Batch 135/468... Discriminator Loss: 0.4589... Generator Loss: 0.3707\n",
      "Epoch 1/25... Batch 136/468... Discriminator Loss: 0.3137... Generator Loss: 1.2163\n",
      "Epoch 1/25... Batch 137/468... Discriminator Loss: 0.3335... Generator Loss: 0.7357\n",
      "Epoch 1/25... Batch 138/468... Discriminator Loss: 0.2904... Generator Loss: 0.5097\n",
      "Epoch 1/25... Batch 139/468... Discriminator Loss: 0.3305... Generator Loss: 1.2589\n",
      "Epoch 1/25... Batch 140/468... Discriminator Loss: 0.2447... Generator Loss: 1.6789\n",
      "Epoch 1/25... Batch 141/468... Discriminator Loss: 0.2245... Generator Loss: 1.6740\n",
      "Epoch 1/25... Batch 142/468... Discriminator Loss: 0.2186... Generator Loss: 1.6353\n",
      "Epoch 1/25... Batch 143/468... Discriminator Loss: 0.2200... Generator Loss: 1.6881\n",
      "Epoch 1/25... Batch 144/468... Discriminator Loss: 0.2444... Generator Loss: 1.4853\n",
      "Epoch 1/25... Batch 145/468... Discriminator Loss: 0.2579... Generator Loss: 1.2343\n",
      "Epoch 1/25... Batch 146/468... Discriminator Loss: 0.2608... Generator Loss: 0.9959\n",
      "Epoch 1/25... Batch 147/468... Discriminator Loss: 0.2690... Generator Loss: 0.6514\n",
      "Epoch 1/25... Batch 148/468... Discriminator Loss: 0.2728... Generator Loss: 0.5150\n",
      "Epoch 1/25... Batch 149/468... Discriminator Loss: 0.2584... Generator Loss: 0.3281\n",
      "Epoch 1/25... Batch 150/468... Discriminator Loss: 0.2533... Generator Loss: 0.3878\n",
      "Epoch 1/25... Batch 151/468... Discriminator Loss: 0.2402... Generator Loss: 0.3755\n",
      "Epoch 1/25... Batch 152/468... Discriminator Loss: 0.2290... Generator Loss: 0.3287\n",
      "Epoch 1/25... Batch 153/468... Discriminator Loss: 0.2401... Generator Loss: 0.2554\n",
      "Epoch 1/25... Batch 154/468... Discriminator Loss: 0.2327... Generator Loss: 0.3243\n",
      "Epoch 1/25... Batch 155/468... Discriminator Loss: 0.2171... Generator Loss: 0.3159\n",
      "Epoch 1/25... Batch 156/468... Discriminator Loss: 0.2416... Generator Loss: 0.2039\n",
      "Epoch 1/25... Batch 157/468... Discriminator Loss: 0.2327... Generator Loss: 0.2379\n",
      "Epoch 1/25... Batch 158/468... Discriminator Loss: 0.2263... Generator Loss: 0.2647\n",
      "Epoch 1/25... Batch 159/468... Discriminator Loss: 0.2451... Generator Loss: 0.1675\n",
      "Epoch 1/25... Batch 160/468... Discriminator Loss: 0.2537... Generator Loss: 0.2848\n",
      "Epoch 1/25... Batch 161/468... Discriminator Loss: 0.2350... Generator Loss: 0.2247\n",
      "Epoch 1/25... Batch 162/468... Discriminator Loss: 0.2203... Generator Loss: 0.1919\n",
      "Epoch 1/25... Batch 163/468... Discriminator Loss: 0.2389... Generator Loss: 0.2071\n",
      "Epoch 1/25... Batch 164/468... Discriminator Loss: 0.2295... Generator Loss: 0.1866\n",
      "Epoch 1/25... Batch 165/468... Discriminator Loss: 0.2545... Generator Loss: 0.1608\n",
      "Epoch 1/25... Batch 166/468... Discriminator Loss: 0.2578... Generator Loss: 0.2640\n",
      "Epoch 1/25... Batch 167/468... Discriminator Loss: 0.2733... Generator Loss: 0.1758\n",
      "Epoch 1/25... Batch 168/468... Discriminator Loss: 0.2632... Generator Loss: 0.2770\n",
      "Epoch 1/25... Batch 169/468... Discriminator Loss: 0.2733... Generator Loss: 0.2630\n",
      "Epoch 1/25... Batch 170/468... Discriminator Loss: 0.2769... Generator Loss: 0.3173\n",
      "Epoch 1/25... Batch 171/468... Discriminator Loss: 0.2617... Generator Loss: 0.5270\n",
      "Epoch 1/25... Batch 172/468... Discriminator Loss: 0.2736... Generator Loss: 0.4741\n",
      "Epoch 1/25... Batch 173/468... Discriminator Loss: 0.3111... Generator Loss: 0.6012\n",
      "Epoch 1/25... Batch 174/468... Discriminator Loss: 0.2982... Generator Loss: 1.1354\n",
      "Epoch 1/25... Batch 175/468... Discriminator Loss: 0.2796... Generator Loss: 1.0339\n",
      "Epoch 1/25... Batch 176/468... Discriminator Loss: 0.2850... Generator Loss: 1.5993\n",
      "Epoch 1/25... Batch 177/468... Discriminator Loss: 0.2477... Generator Loss: 1.4307\n",
      "Epoch 1/25... Batch 178/468... Discriminator Loss: 0.2359... Generator Loss: 1.5779\n",
      "Epoch 1/25... Batch 179/468... Discriminator Loss: 0.2391... Generator Loss: 1.6937\n",
      "Epoch 1/25... Batch 180/468... Discriminator Loss: 0.2335... Generator Loss: 1.7272\n",
      "Epoch 1/25... Batch 181/468... Discriminator Loss: 0.2362... Generator Loss: 1.8793\n",
      "Epoch 1/25... Batch 182/468... Discriminator Loss: 0.2339... Generator Loss: 1.7572\n",
      "Epoch 1/25... Batch 183/468... Discriminator Loss: 0.2466... Generator Loss: 1.9422\n",
      "Epoch 1/25... Batch 184/468... Discriminator Loss: 0.2592... Generator Loss: 1.4090\n",
      "Epoch 1/25... Batch 185/468... Discriminator Loss: 0.3347... Generator Loss: 1.8051\n",
      "Epoch 1/25... Batch 186/468... Discriminator Loss: 0.3993... Generator Loss: 0.8280\n",
      "Epoch 1/25... Batch 187/468... Discriminator Loss: 0.5465... Generator Loss: 1.5456\n",
      "Epoch 1/25... Batch 188/468... Discriminator Loss: 0.2714... Generator Loss: 1.4348\n",
      "Epoch 1/25... Batch 189/468... Discriminator Loss: 0.2384... Generator Loss: 1.3548\n",
      "Epoch 1/25... Batch 190/468... Discriminator Loss: 0.2488... Generator Loss: 1.8242\n",
      "Epoch 1/25... Batch 191/468... Discriminator Loss: 0.2213... Generator Loss: 2.2201\n",
      "Epoch 1/25... Batch 192/468... Discriminator Loss: 0.2260... Generator Loss: 2.4372\n",
      "Epoch 1/25... Batch 193/468... Discriminator Loss: 0.2291... Generator Loss: 2.7998\n",
      "Epoch 1/25... Batch 194/468... Discriminator Loss: 0.2376... Generator Loss: 3.4800\n",
      "Epoch 1/25... Batch 195/468... Discriminator Loss: 0.2367... Generator Loss: 4.1692\n",
      "Epoch 1/25... Batch 196/468... Discriminator Loss: 0.2095... Generator Loss: 4.7254\n",
      "Epoch 1/25... Batch 197/468... Discriminator Loss: 0.1944... Generator Loss: 5.1221\n",
      "Epoch 1/25... Batch 198/468... Discriminator Loss: 0.2041... Generator Loss: 5.1174\n",
      "Epoch 1/25... Batch 199/468... Discriminator Loss: 0.1908... Generator Loss: 5.1414\n",
      "Epoch 1/25... Batch 200/468... Discriminator Loss: 0.1890... Generator Loss: 5.2983\n",
      "Epoch 1/25... Batch 201/468... Discriminator Loss: 0.1906... Generator Loss: 5.3740\n",
      "Epoch 1/25... Batch 202/468... Discriminator Loss: 0.1949... Generator Loss: 5.4632\n",
      "Epoch 1/25... Batch 203/468... Discriminator Loss: 0.2061... Generator Loss: 5.6087\n",
      "Epoch 1/25... Batch 204/468... Discriminator Loss: 0.2202... Generator Loss: 5.6354\n",
      "Epoch 1/25... Batch 205/468... Discriminator Loss: 0.2234... Generator Loss: 5.4893\n",
      "Epoch 1/25... Batch 206/468... Discriminator Loss: 0.2403... Generator Loss: 5.0099\n",
      "Epoch 1/25... Batch 207/468... Discriminator Loss: 0.2626... Generator Loss: 4.7713\n",
      "Epoch 1/25... Batch 208/468... Discriminator Loss: 0.2896... Generator Loss: 4.2574\n",
      "Epoch 1/25... Batch 209/468... Discriminator Loss: 0.3311... Generator Loss: 4.8099\n",
      "Epoch 1/25... Batch 210/468... Discriminator Loss: 0.5271... Generator Loss: 2.6662\n",
      "Epoch 1/25... Batch 211/468... Discriminator Loss: 1.5049... Generator Loss: 4.7195\n",
      "Epoch 1/25... Batch 212/468... Discriminator Loss: 0.5601... Generator Loss: 3.3863\n",
      "Epoch 1/25... Batch 213/468... Discriminator Loss: 0.2646... Generator Loss: 2.9454\n",
      "Epoch 1/25... Batch 214/468... Discriminator Loss: 0.3262... Generator Loss: 3.3007\n",
      "Epoch 1/25... Batch 215/468... Discriminator Loss: 0.3303... Generator Loss: 2.4395\n",
      "Epoch 1/25... Batch 216/468... Discriminator Loss: 0.3074... Generator Loss: 2.4711\n",
      "Epoch 1/25... Batch 217/468... Discriminator Loss: 0.3134... Generator Loss: 2.5534\n",
      "Epoch 1/25... Batch 218/468... Discriminator Loss: 0.3363... Generator Loss: 2.1742\n",
      "Epoch 1/25... Batch 219/468... Discriminator Loss: 0.3751... Generator Loss: 3.2897\n",
      "Epoch 1/25... Batch 220/468... Discriminator Loss: 0.4160... Generator Loss: 1.9757\n",
      "Epoch 1/25... Batch 221/468... Discriminator Loss: 0.4358... Generator Loss: 3.3145\n",
      "Epoch 1/25... Batch 222/468... Discriminator Loss: 0.4017... Generator Loss: 2.1976\n",
      "Epoch 1/25... Batch 223/468... Discriminator Loss: 0.4040... Generator Loss: 3.3355\n",
      "Epoch 1/25... Batch 224/468... Discriminator Loss: 0.4283... Generator Loss: 1.7345\n",
      "Epoch 1/25... Batch 225/468... Discriminator Loss: 0.5250... Generator Loss: 3.7707\n",
      "Epoch 1/25... Batch 226/468... Discriminator Loss: 0.6828... Generator Loss: 0.8348\n",
      "Epoch 1/25... Batch 227/468... Discriminator Loss: 0.8445... Generator Loss: 3.1564\n",
      "Epoch 1/25... Batch 228/468... Discriminator Loss: 0.6618... Generator Loss: 1.1774\n",
      "Epoch 1/25... Batch 229/468... Discriminator Loss: 0.5598... Generator Loss: 2.3025\n",
      "Epoch 1/25... Batch 230/468... Discriminator Loss: 0.4139... Generator Loss: 1.7437\n",
      "Epoch 1/25... Batch 231/468... Discriminator Loss: 0.3541... Generator Loss: 1.4118\n",
      "Epoch 1/25... Batch 232/468... Discriminator Loss: 0.3644... Generator Loss: 1.6133\n",
      "Epoch 1/25... Batch 233/468... Discriminator Loss: 0.3532... Generator Loss: 1.3444\n",
      "Epoch 1/25... Batch 234/468... Discriminator Loss: 0.3553... Generator Loss: 1.4513\n",
      "Epoch 1/25... Batch 235/468... Discriminator Loss: 0.3629... Generator Loss: 1.4447\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f8c775eec7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmake_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mgen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mmake_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 25\n",
    "num_batches = int(x_train.shape[0] / batch_size)\n",
    "\n",
    "for e in range(epochs):\n",
    "    for b in range(num_batches):\n",
    "        # noise and train images\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, z_size]).astype(np.float32)\n",
    "        images = x_train[b*batch_size:(b + 1)*batch_size]\n",
    "        \n",
    "        # generate image\n",
    "        gen_images = generator.predict(noise, verbose=0)\n",
    "        \n",
    "        # save generated images periodically\n",
    "        if ((b+1) % 50) == 0:\n",
    "            combined_image = combine_images(gen_images)\n",
    "            combined_image = combined_image*127.5+127.5\n",
    "            Image.fromarray(combined_image.astype(np.uint8)) \\\n",
    "                .save(\"{}/{}_{}.png\".format(GEN_IMAGES_DIR, e+1, b+1))\n",
    "        \n",
    "        # calcualate discriminator loss\n",
    "        discr_input = np.concatenate((images, gen_images))\n",
    "        discr_labels = [0.9]*batch_size + [0.0]*batch_size \n",
    "        d_loss = discriminator.train_on_batch(discr_input, discr_labels)\n",
    "        \n",
    "        # calculate generator loss\n",
    "        make_trainable(discriminator, False)\n",
    "        gen_labels = [1]*batch_size\n",
    "        g_loss = gan.train_on_batch(noise, gen_labels)\n",
    "        make_trainable(discriminator, True)\n",
    "        \n",
    "        # print statistic\n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Batch {}/{}...\".format(b+1, num_batches),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(d_loss),\n",
    "              \"Generator Loss: {:.4f}\".format(g_loss))\n",
    "        \n",
    "        # save weights periodically\n",
    "        if ((b+1) % 10) == 0:\n",
    "            generator.save_weights('generator', True)\n",
    "            discriminator.save_weights('discriminator', True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create directory for generated images\n",
    "GEN_IMAGES_DIR_FINAL = 'gen_images_final'\n",
    "if not os.path.exists(GEN_IMAGES_DIR_FINAL):\n",
    "    os.makedirs(GEN_IMAGES_DIR_FINAL)\n",
    "    \n",
    "# create generator and load its weights\n",
    "generator = generator_model(z_size)\n",
    "generator.compile(optimizer=adam_optimizer, loss='binary_crossentropy')\n",
    "generator.load_weights('generator')\n",
    "    \n",
    "# generate images\n",
    "NUM_GEN_IMAGES = 64\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[NUM_GEN_IMAGES, z_size]).astype(np.float32)\n",
    "gen_images = generator.predict(noise, verbose=0)\n",
    "\n",
    "# save images\n",
    "combined_image = combine_images(gen_images)\n",
    "combined_image = combined_image*127.5+127.5\n",
    "Image.fromarray(combined_image.astype(np.uint8)) \\\n",
    "    .save(\"{}/gen_images.png\".format(GEN_IMAGES_DIR_FINAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# show generated image\n",
    "plt.imshow(combined_image, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
